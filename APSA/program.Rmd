---
title: "Scrape the APSA 2020 online program with `rvest`"
subtitle: "(because we are nerds who want to play pictionary)"
author: 
output:
    html_document:
      highlight: zenburn
      #toc: true
      #toc_float: true
      #code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{r options, echo = FALSE, warning=FALSE, message = FALSE, code_folding = "hide"}
## Modify some defaults for this example
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE)
library(magrittr)
library(tidyverse)
```

# Motivation

Devon Cantwell had a fun idea.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Anyone interested in a virtual happy hour for grad students at APSA in a few weeks? I can organize a few games and we can keep it to like an hour or so! You wouldn‚Äôt have to be presenting or registered to participate either!</p>&mdash; Devon Cantwell (@devon_cantwell) <a href="https://twitter.com/devon_cantwell/status/1297125769773293569?ref_src=twsrc%5Etfw">August 22, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

This reminded me of playing Pictionary with my research lab.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">After lab meetings, we play <a href="https://t.co/ILwcWbPhT9">https://t.co/ILwcWbPhT9</a> Pictionary üßëüé®üñºÔ∏è using words from our data:<br><br>d %&gt;%<br> unnest_tokens(word, text) %&gt;% <a href="https://twitter.com/hashtag/tidytext?src=hash&amp;ref_src=twsrc%5Etfw">#tidytext</a>!<br> inner_join(get_sentiments(&quot;nrc&quot;)) %&gt;%<br> count(word) %&gt;% <br> slice_sample(n = 100, weight_by = n) %&gt;%<br> .$word %&gt;%<br> str_c(collapse = &quot;, &quot;) <a href="https://t.co/MIobNupp03">pic.twitter.com/MIobNupp03</a></p>&mdash; Devin Judge-Lord (@JudgeLord) <a href="https://twitter.com/JudgeLord/status/1275158859485773824?ref_src=twsrc%5Etfw">June 22, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



# Methods

## Let's grab some words from the APSA 2020 program.

Scraping websites with `rvest` is easy! (I'll also use `tidyverse` and `magritter` functions here.)

A fun minimal example is the UN website: 
```{r un, attr.output='style="max-height: 100px;"'}
library(rvest)

html <- read_html("https://UN.org") # The UN homepage
links <- html_nodes(html, "a") # "a" nodes are linked text
html_text(links)
```


APSA complicates things by making us select a timezone:
```{r apsa, attr.output='style="max-height: 100px;"'}
url <- "https://convention2.allacademic.com/one/apsa/apsa20/"

read_html(url) %>% html_text()
```

So, we must set a timezone for our session. `rvest` has several tools that allow us to submit web forms. 
```{r session}
mysession <- html_session(url)

timzone_form <- html_form(mysession)[[1]] %>% 
  set_values(new_timezone = "Africa/Abidjan")

submit_form(mysession, timzone_form)
```

After providing our newly-created browser session info, we can now navitgate to APSA's "Created Panels" page with `rvest`'s `follow_link` function. We can then read the html and grab the linked text nodes. 
```{r navigate}
html <- jump_to(mysession, url) %>% 
  follow_link("Browse By Session or Event Type") %>%
  follow_link("Created Panel") %>%
  read_html()

links <- html_nodes(html, "a") # "a" nodes are linked text

html_text(links) %>% head(20)
```

`html_text` extracts text from HTML nodes. On this page the linked text is the title of each panel (except for the first 14 links). 

To clean up the panel titles, I remove all text before "TBA" or after "Sub Unit" using the one [regular expression](https://stringr.tidyverse.org/articles/regular-expressions.html) to rule them all `.*`, which matches anything (`.`) anynumber of times (`*`).

`html_attr` extracts other HTML attributes. Linked URLs are in the "href" attribute.

Let's put both into a tidy dataframe:
```{r dataframe}
d <- tibble(title = html_text(links) %>% 
              str_remove_all(".*TBA|Sub Unit.*"),
            url = html_attr(links, "href") 
            )

# filter to rows that contain a "session_id" in their URL
d %<>% filter( str_detect(url, "session_id") )

d
```

# Results

Now that we have a tidy dataframe with a column of text, the world is our oyster. We could follow each URL to get more details on each panel using `purrr`s `map_dfr` like I did [here](https://judgelord.github.io/correspondence/functions/DOE_FERC-company-scraper.html), but I should get back to writing my APSA paper.

For pictionary, we just need a sample of common words. The `tidytext` package has a number of helpful tools for doing this. Most importantly, `unnest_tokens` "tokenizes" text--here breaking it up by word. `filter`, `count`, and `slice` from `dplyr` help us clean up, sample, and collapse these words into a block of text.

> Tip: to get drawable words from messier text, try keeping only words in the NRC dictionary by adding `inner_join(get_sentiments("nrc"))` anywhere between unnesting and sampling them. 

```{r words, attr.output='style="max-height: 700px;"'}
library(tidytext)

word_counts <- d %>%
  # get words from a column "title"
  unnest_tokens(word, title) %>% 
  # remove common words (such as "the")
  anti_join(stop_words) %>%
  # filter out words less than 5 letters or with apostrophes
  filter( nchar(word) > 5, !str_detect(word, "\\'") ) %>%
  # sample 500 words, weighted by their frequency
  count(word)

word_counts %>% arrange(-n)

word_counts %>% 
  slice_sample(n = 500, weight_by = n) %>%
  # collapse to a block of text, separating words with commas
  .$word %>%
  str_c(collapse = ", ")
```

<!--

# APSA papers on created panels

```{r papers}
knitr::kable(d)
```
-->